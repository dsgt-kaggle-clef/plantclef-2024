{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Embedding using DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/31 19:29:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/31 19:29:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee4410ab60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=4, memory=\"8g\")\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 31 19:29:14 UTC 2024\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2022_web_training_images_1/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2022_web_training_images_4/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2024_test/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2024_training/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2024_training_cropped_resized/\n",
      "gs://dsgt-clef-plantclef-2024/data/parquet_files/PlantCLEF2024_training_cropped_resized_v2/\n"
     ]
    }
   ],
   "source": [
    "# Get list of stored filed in cloud bucket\n",
    "root = \"gs://dsgt-clef-plantclef-2024\"\n",
    "! date\n",
    "! gcloud storage ls {root}/data/parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------+----------+--------------------+-------+--------------------+--------+-------------+-------------+---------------+--------------------+----------+-------------+--------+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|          image_name|                path| organ|species_id|    obs_id|             license|partner|              author|altitude|     latitude|    longitude|gbif_species_id|             species|     genus|       family| dataset|  publisher|          references|                 url|learn_tag|    image_backup_url|                data|\n",
      "+--------------------+--------------------+------+----------+----------+--------------------+-------+--------------------+--------+-------------+-------------+---------------+--------------------+----------+-------------+--------+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|2fb34c40832bffad3...|/PlantCLEF2024/tr...|  bark|   1361703|4116028543|http://creativeco...|   NULL|              Tony H|    NULL|         NULL|         NULL|      5328663.0|Posidonia oceanic...| Posidonia|Posidoniaceae|    gbif|iNaturalist|https://www.inatu...|https://inaturali...|    train|https://lab.plant...|[FF D8 FF E0 00 1...|\n",
      "|38da078be8660b772...|/PlantCLEF2024/tr...| habit|   1355927|1003213237|            cc-by-sa|   tela|Tela Botanica âˆ’ J...|   219.0|     43.59753|     -8.12915|      3114986.0|Arctotheca calend...|Arctotheca|   Asteraceae|plantnet|   plantnet|https://identify....|https://bs.plantn...|    train|https://lab.plant...|[FF D8 FF E0 00 1...|\n",
      "|4fe98ed9eff6eed40...|/PlantCLEF2024/tr...|flower|   1388692|1009573842|            cc-by-sa|   NULL|   Monteiro Henrique|   124.0|38.6945228605|-9.2953275237|      3172592.0|Podranea ricasoli...|  Podranea| Bignoniaceae|plantnet|   plantnet|https://identify....|https://bs.plantn...|    train|https://lab.plant...|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+----------+----------+--------------------+-------+--------------------+--------+-------------+-------------+---------------+--------------------+----------+-------------+--------+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/parquet_files\"\n",
    "train = \"PlantCLEF2024_training_cropped_resized_v2\"\n",
    "\n",
    "# Define the GCS path to the Train parquet file\n",
    "train_gcs_path = f\"{gcs_path}/{train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "train_df = spark.read.parquet(train_gcs_path)\n",
    "\n",
    "# Show the data (for example, first few rows)\n",
    "train_df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|          image_name|species_id|           dctn_data|\n",
      "+--------------------+----------+--------------------+\n",
      "|b4660ccafa8567718...|   1361703|[-15065.38, -446....|\n",
      "|d61f7d5ba5a3554cb...|   1361703|[-20283.373, 3034...|\n",
      "|24d80097e70d5f914...|   1361703|[-17341.75, -1007...|\n",
      "|d4dc1b782195687c2...|   1355927|[-26710.031, -457...|\n",
      "|9c51869fcb57794f9...|   1361703|[-28332.748, -205...|\n",
      "|e6c09450ef071b82b...|   1355927|[-30168.426, 1062...|\n",
      "|bddb8be8e7927aa58...|   1361703|[-29858.95, 31081...|\n",
      "|5937bba593427705f...|   1355927|[-24212.04, 17660...|\n",
      "|915978130f13a5fc2...|   1355927|[-25134.674, -847...|\n",
      "|7e91e5a4c7780887e...|   1355927|[-29754.826, 5821...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from plantclef.transforms import WrappedDinoV2, DCTN\n",
    "\n",
    "# Get subset of images to test pipeline\n",
    "train100_df = (\n",
    "    train_df.where(F.col(\"species_id\").isin([1361703, 1355927]))\n",
    "    .orderBy(F.rand(1000))\n",
    "    .limit(200)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Init DINOv2 wrapper\n",
    "dino = WrappedDinoV2(input_col=\"data\", output_col=\"transformed_data\")\n",
    "\n",
    "# Init Descrite Cosine Transform wrapper\n",
    "dctn = DCTN(input_col=\"transformed_data\", output_col=\"dctn_data\")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[dino, dctn])\n",
    "\n",
    "# Fit pipeline to DF\n",
    "model = pipeline.fit(train100_df)\n",
    "\n",
    "# Apply the model to transform the DF\n",
    "transformed_df = model.transform(train100_df).cache()\n",
    "\n",
    "# Show results\n",
    "transformed_df.select([\"image_name\", \"species_id\", \"dctn_data\"]).show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|species_id|count|\n",
      "+----------+-----+\n",
      "|   1361703|  108|\n",
      "|   1355927|   92|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.groupBy(\"species_id\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "# Load training data\n",
    "training = transformed_df.select(\n",
    "    array_to_vector(\"dctn_data\").alias(\"dctn_data\"), \"image_name\", \"species_id\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "lr_pipe = Pipeline(\n",
    "    stages=[\n",
    "        LogisticRegression(\n",
    "            featuresCol=\"dctn_data\",\n",
    "            labelCol=\"species_id\",\n",
    "            maxIter=10,\n",
    "            regParam=0.3,\n",
    "            elasticNetParam=0.8,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr_pipe.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\n",
    "    \"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "    % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
