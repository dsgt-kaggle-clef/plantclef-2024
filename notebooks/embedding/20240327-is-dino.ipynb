{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/istalter/plantclef-2024/'\n",
      "/Users/istalter/Desktop/plantclef-2024/notebooks/embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/istalter/Library/Python/3.11/lib/python/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plantclef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/istalter/plantclef-2024/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplantclef\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_spark\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# https://knowledge.informatica.com/s/article/000196886?language=en_US\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# The vectorized reader will run out of memory (8gb) with the default batch size, so\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# this is one way of handling the issue. This is likely due to the fact that the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# column is so damn big, and treated as binary data instead of something like a string.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# We might also be able to avoid this if we don't cache the fields into memory, but this\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# this needs to be validated by hand. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m get_spark(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# \"spark.sql.parquet.columnarReaderBatchSize\": 512,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.parquet.enableVectorizedReader\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     13\u001b[0m })\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plantclef'"
     ]
    }
   ],
   "source": [
    "%cd /home/istalter/plantclef-2024/\n",
    "from plantclef.utils import get_spark\n",
    "\n",
    "# https://knowledge.informatica.com/s/article/000196886?language=en_US\n",
    "# The vectorized reader will run out of memory (8gb) with the default batch size, so\n",
    "# this is one way of handling the issue. This is likely due to the fact that the data\n",
    "# column is so damn big, and treated as binary data instead of something like a string.\n",
    "# We might also be able to avoid this if we don't cache the fields into memory, but this\n",
    "# this needs to be validated by hand. \n",
    "spark = get_spark(**{\n",
    "    # \"spark.sql.parquet.columnarReaderBatchSize\": 512,\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "\n",
    "size = 'small' # small, medium, large\n",
    "gcs_parquet_path = \"gs://dsgt-clef-plantclef-2024/data/parquet_files/\"\n",
    "input_folder = f\"PlantCLEF2024_training_cropped_resized_v2/\"\n",
    "\n",
    "df = spark.read.parquet(gcs_parquet_path+input_folder)\n",
    "df.printSchema()\n",
    "# df.show(1, vertical=True, truncate=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_batch_udf\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryType, ArrayType, FloatType\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import BinaryType, ArrayType, FloatType\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def make_predict_fn():\n",
    "    \"\"\"Return PredictBatchFunction\"\"\"\n",
    "    from transformers import AutoImageProcessor, AutoModel\n",
    "    ### need to update this with the pretrained dino\n",
    "    processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "    model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        # print('inputs:')\n",
    "        # print(type(inputs))\n",
    "        # print(inputs.shape)\n",
    "\n",
    "        images = [Image.open(io.BytesIO(input)) for input in inputs]\n",
    "        # print('images:')\n",
    "        # print(type(images))\n",
    "        # print(images)\n",
    "\n",
    "        model_inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        # print('model_inputs:')\n",
    "        # print(type(model_inputs))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # print('start modeling')\n",
    "            outputs = model(**model_inputs)\n",
    "            # print('outputs')\n",
    "            # print(outputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # print('last_hidden_states:')\n",
    "        # print(type(last_hidden_states))\n",
    "        # print(last_hidden_states.shape)\n",
    "\n",
    "        numpy_array = last_hidden_states.numpy()\n",
    "        # Reshape the array\n",
    "        new_shape = numpy_array.shape[:-2] + (-1,)\n",
    "        numpy_array = numpy_array.reshape(new_shape)\n",
    "\n",
    "        # print('numpy_array:')\n",
    "        # print(type(numpy_array))\n",
    "        # print(numpy_array.shape)\n",
    "\n",
    "        return numpy_array\n",
    "\n",
    "    return predict\n",
    "    \n",
    "# batch prediction UDF\n",
    "apply_dino_pbudf = predict_batch_udf(\n",
    "    make_predict_fn = make_predict_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to transform images\n",
    "df_transformed = df.limit(24).withColumn(\"transformed_image\", apply_dino_pbudf(df[\"data\"]))\n",
    "\n",
    "df_transformed.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
