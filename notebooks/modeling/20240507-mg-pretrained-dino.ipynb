{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DinoV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make directory to store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p /mnt/data/models\n",
    "# %cd /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the file from GCS to `/mnt/data/models` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp gs://dsgt-clef-plantclef-2024/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar /mnt/data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the `.tar` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xvf /mnt/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar -C /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use pretrained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=\";\", quoting=1, dtype={\"species_id\": str})\n",
    "    df = df.set_index(\"species_id\")\n",
    "    return df[\"species\"].to_dict()\n",
    "\n",
    "\n",
    "def main(image_url, class_mapping, species_mapping, pretrained_path):\n",
    "    cid_to_spid = load_class_mapping(class_mapping)\n",
    "    spid_to_sp = load_species_mapping(species_mapping)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = timm.create_model(\n",
    "        \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        pretrained=False,\n",
    "        num_classes=len(cid_to_spid),\n",
    "        checkpoint_path=pretrained_path,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    img = None\n",
    "    if \"https://\" in image_url or \"http://\" in image_url:\n",
    "        img = Image.open(urlopen(image_url))\n",
    "    elif image_url != None:\n",
    "        img = Image.open(image_url)\n",
    "\n",
    "    if img != None:\n",
    "        img = transforms(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)  # unsqueeze single image into batch of 1\n",
    "        top5_probabilities, top5_class_indices = torch.topk(\n",
    "            output.softmax(dim=1) * 100, k=5\n",
    "        )\n",
    "        top5_probabilities = top5_probabilities.cpu().detach().numpy()\n",
    "        top5_class_indices = top5_class_indices.cpu().detach().numpy()\n",
    "\n",
    "        for proba, cid in zip(top5_probabilities[0], top5_class_indices[0]):\n",
    "            species_id = cid_to_spid[cid]\n",
    "            species = spid_to_sp[species_id]\n",
    "            print(species_id, species, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths and image URL directly\n",
    "path = \"/mnt/data/models/pretrained_models\"\n",
    "image_url = \"https://lab.plantnet.org/LifeCLEF/PlantCLEF2024/single_plant_training_data/PlantCLEF2024singleplanttrainingdata/test/1361687/bd2d3830ac3270218ba82fd24e2290becd01317c.jpg\"\n",
    "class_mapping_file = f\"{path}/class_mapping.txt\"\n",
    "species_mapping_file = f\"{path}/species_id_to_name.txt\"\n",
    "model_path = \"vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all\"\n",
    "pretrained_path = f\"{path}/{model_path}/model_best.pth.tar\"\n",
    "\n",
    "main(image_url, class_mapping_file, species_mapping_file, pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 14:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/11 14:26:43 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7d3a3cd0e800>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "|                                       path|              image_name|                                              data|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "| /PlantCLEF2024test/CBN-Pla-B4-20160728.jpg| CBN-Pla-B4-20160728.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D3-20130808.jpg| CBN-Pla-D3-20130808.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "|/PlantCLEF2024test/CBN-PdlC-E4-20150701.jpg|CBN-PdlC-E4-20150701.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-F5-20150901.jpg| CBN-Pla-F5-20150901.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D1-20180724.jpg| CBN-Pla-D1-20180724.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get dataframes\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "test_data_path = \"data/parquet_files/PlantCLEF2024_test\"\n",
    "\n",
    "# paths to dataframe\n",
    "test_path = f\"{gcs_path}/{test_data_path}\"\n",
    "# read data\n",
    "test_df = spark.read.parquet(test_path)\n",
    "# show\n",
    "test_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df = test_df.limit(10).cache()\n",
    "limit_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, MapType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from plantclef.model_setup import setup_pretrained_model\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import vector_to_array, predict_batch_udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedDinoV2(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path: str,\n",
    "        input_col: str = \"input\",\n",
    "        output_col: str = \"output\",\n",
    "        model_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(inputCol=input_col, outputCol=output_col)\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.num_classes = 7806  # total number of plant species\n",
    "        self.local_directory = \"/mnt/data/models/pretrained_models\"\n",
    "        self.class_mapping_file = f\"{self.local_directory}/class_mapping.txt\"\n",
    "        # Model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = timm.create_model(\n",
    "            self.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=self.num_classes,\n",
    "            checkpoint_path=self.pretrained_path,\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # Data transform\n",
    "        self.data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        self.transforms = timm.data.create_transform(\n",
    "            **self.data_config, is_training=False\n",
    "        )\n",
    "        self.sql_statement = \"SELECT image_name, dino_logits FROM __THIS__\"\n",
    "        self.cid_to_spid = self._load_class_mapping()\n",
    "\n",
    "    def _load_class_mapping(self):\n",
    "        with open(self.class_mapping_file) as f:\n",
    "            class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "        return class_index_to_class_name\n",
    "\n",
    "    def _make_predict_fn(self):\n",
    "        \"\"\"Return PredictBatchFunction using a closure over the model\"\"\"\n",
    "\n",
    "        def predict(input_data: list) -> list:\n",
    "            # Load all inputs into a batch of images\n",
    "            img = Image.open(io.BytesIO(input_data))\n",
    "            # Transform and stack images to a single tensor\n",
    "            processed_image = self.transforms(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(processed_image)\n",
    "                probabilities = torch.softmax(outputs, dim=1) * 100\n",
    "                top_probs, top_indices = torch.topk(probabilities, k=20)\n",
    "\n",
    "            top_probs = top_probs.cpu().numpy()[0]\n",
    "            top_indices = top_indices.cpu().numpy()[0]\n",
    "\n",
    "            # Convert top indices and probabilities to a dictionary\n",
    "            result = [\n",
    "                {self.cid_to_spid.get(index, \"Unknown\"): float(prob)}\n",
    "                for index, prob in zip(top_indices, top_probs)\n",
    "            ]\n",
    "            return result\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Create a UDF from the predict function\n",
    "        predict_fn = self._make_predict_fn()\n",
    "        predict_udf = F.udf(predict_fn, ArrayType(MapType(StringType(), FloatType())))\n",
    "\n",
    "        return df.withColumn(\n",
    "            self.getOutputCol(),\n",
    "            predict_udf(self.getInputCol()),\n",
    "        )\n",
    "\n",
    "    def transform(self, df) -> DataFrame:\n",
    "        transformed = self._transform(df)\n",
    "\n",
    "        for c in self.feature_columns:\n",
    "            # check if the feature is a vector and convert it to an array\n",
    "            if \"array\" in transformed.schema[c].simpleString():\n",
    "                continue\n",
    "            transformed = transformed.withColumn(c, vector_to_array(F.col(c)))\n",
    "        return transformed\n",
    "\n",
    "    @property\n",
    "    def feature_columns(self) -> list:\n",
    "        return [\"dino_logits\"]\n",
    "\n",
    "    def pipeline(self):\n",
    "        return Pipeline(stages=[self, SQLTransformer(statement=self.sql_statement)])\n",
    "\n",
    "    def run(self, df: DataFrame) -> DataFrame:\n",
    "        model = self.pipeline().fit(df)\n",
    "        transformed = model.transform(df)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = setup_pretrained_model()\n",
    "pretrained_dino = PretrainedDinoV2(\n",
    "    pretrained_path=pretrained_path,\n",
    "    input_col=\"data\",\n",
    "    output_col=\"dino_logits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "results: [{'1361281': 7.845359802246094}, {'1741880': 4.131918430328369}, {'1741903': 2.7131903171539307}, {'1397070': 2.321333169937134}, {'1362271': 2.125537157058716}, {'1388766': 1.5371482372283936}, {'1395807': 1.430025577545166}, {'1363407': 1.3783882856369019}, {'1396640': 1.1620782613754272}, {'1358379': 0.8249717950820923}, {'1544716': 0.7349836826324463}, {'1391317': 0.6994383931159973}, {'1390929': 0.6965225338935852}, {'1390899': 0.6008772850036621}, {'1519650': 0.5890170335769653}, {'1394999': 0.515813410282135}, {'1396869': 0.48324480652809143}, {'1389059': 0.48096683621406555}, {'1395868': 0.44235530495643616}, {'1395862': 0.4272904694080353}]\n",
      "results: [{'1741706': 6.709542274475098}, {'1741903': 5.509812831878662}, {'1741880': 3.1866233348846436}, {'1737559': 2.670597553253174}, {'1395862': 2.496324300765991}, {'1397524': 2.423849582672119}, {'1361281': 2.0336947441101074}, {'1397070': 1.7238483428955078}, {'1395807': 1.6485413312911987}, {'1363266': 1.6216201782226562}, {'1394999': 1.5012688636779785}, {'1360269': 0.9212446212768555}, {'1414270': 0.7909188270568848}, {'1369309': 0.7414475679397583}, {'1395870': 0.738686203956604}, {'1478563': 0.7195161581039429}, {'1666930': 0.7086296677589417}, {'1364147': 0.6351821422576904}, {'1397535': 0.6347458362579346}, {'1397565': 0.6230255365371704}]\n",
      "results: [{'1392323': 6.9817914962768555}, {'1361281': 5.23351526260376}, {'1395807': 3.315732717514038}, {'1391313': 3.277189016342163}, {'1389603': 2.0431113243103027}, {'1720959': 1.5904446840286255}, {'1397123': 1.3717807531356812}, {'1390690': 1.2089678049087524}, {'1363476': 1.1094969511032104}, {'1741880': 1.0236284732818604}, {'1741903': 0.8162428736686707}, {'1389346': 0.7080706357955933}, {'1361129': 0.7075967192649841}, {'1394677': 0.6356262564659119}, {'1361372': 0.6246333122253418}, {'1389018': 0.6062524914741516}, {'1397070': 0.5476575493812561}, {'1392407': 0.5363969206809998}, {'1392333': 0.5159096121788025}, {'1397333': 0.4940575063228607}]\n",
      "results: [{'1361281': 14.984538078308105}, {'1390764': 10.862804412841797}, {'1361043': 8.259321212768555}, {'1390846': 2.9599833488464355}, {'1394519': 2.1079351902008057}, {'1389929': 1.5373913049697876}, {'1738640': 1.3843214511871338}, {'1361275': 1.219641923904419}, {'1737699': 1.2069708108901978}, {'1390883': 1.1529923677444458}, {'1720959': 1.0527716875076294}, {'1393691': 1.0516221523284912}, {'1393659': 1.048301339149475}, {'1392323': 0.5781055092811584}, {'1647677': 0.5422883033752441}, {'1743474': 0.5281341671943665}, {'1425722': 0.4256773591041565}, {'1651363': 0.3336857557296753}, {'1362331': 0.32204151153564453}, {'1414387': 0.28368234634399414}]\n",
      "results: [{'1361281': 35.63188552856445}, {'1651363': 21.19072914123535}, {'1390872': 1.5745058059692383}, {'1392323': 1.2649561166763306}, {'1361129': 1.0475839376449585}, {'1737684': 0.5901562571525574}, {'1743474': 0.5457438826560974}, {'1398310': 0.42971304059028625}, {'1394911': 0.3792187571525574}, {'1360352': 0.29408448934555054}, {'1394345': 0.2526407837867737}, {'1394443': 0.2359917163848877}, {'1738640': 0.2344871610403061}, {'1741754': 0.21904131770133972}, {'1398222': 0.21837085485458374}, {'1741589': 0.21644863486289978}, {'1652912': 0.19653376936912537}, {'1361067': 0.1833764761686325}, {'1389650': 0.18091566860675812}, {'1394630': 0.17037704586982727}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------------------------+\n",
      "|              image_name|                                       dino_logits|\n",
      "+------------------------+--------------------------------------------------+\n",
      "| CBN-Pla-B4-20160728.jpg|[{1361281 -> 7.84536}, {1741880 -> 4.1319184}, ...|\n",
      "| CBN-Pla-D3-20130808.jpg|[{1741706 -> 6.7095423}, {1741903 -> 5.509813},...|\n",
      "|CBN-PdlC-E4-20150701.jpg|[{1392323 -> 6.9817915}, {1361281 -> 5.2335153}...|\n",
      "| CBN-Pla-F5-20150901.jpg|[{1361281 -> 14.984538}, {1390764 -> 10.862804}...|\n",
      "| CBN-Pla-D1-20180724.jpg|[{1361281 -> 35.631886}, {1651363 -> 21.19073},...|\n",
      "+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "results: [{'1361281': 7.391123294830322}, {'1743474': 2.836224317550659}, {'1741880': 2.1084747314453125}, {'1394311': 1.8508533239364624}, {'1397463': 1.6546798944473267}, {'1414270': 1.255638599395752}, {'1478563': 0.6917396187782288}, {'1359282': 0.5534758567810059}, {'1398779': 0.5463439226150513}, {'1369068': 0.5071946978569031}, {'1390929': 0.45880624651908875}, {'1396717': 0.45795753598213196}, {'1395807': 0.45595788955688477}, {'1412857': 0.45178186893463135}, {'1397867': 0.4127252399921417}, {'1411089': 0.38960501551628113}, {'1398772': 0.38724297285079956}, {'1361284': 0.3817717730998993}, {'1397495': 0.38029518723487854}, {'1397535': 0.37551528215408325}]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df = pretrained_dino.run(df=limit_df)\n",
    "transformed_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
