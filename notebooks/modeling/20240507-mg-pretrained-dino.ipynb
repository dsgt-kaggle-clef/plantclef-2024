{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DinoV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make directory to store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p /mnt/data/models\n",
    "# %cd /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the file from GCS to `/mnt/data/models` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp gs://dsgt-clef-plantclef-2024/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar /mnt/data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the `.tar` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xvf /mnt/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar -C /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use pretrained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=\";\", quoting=1, dtype={\"species_id\": str})\n",
    "    df = df.set_index(\"species_id\")\n",
    "    return df[\"species\"].to_dict()\n",
    "\n",
    "\n",
    "def main(image_url, class_mapping, species_mapping, pretrained_path):\n",
    "    cid_to_spid = load_class_mapping(class_mapping)\n",
    "    spid_to_sp = load_species_mapping(species_mapping)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = timm.create_model(\n",
    "        \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        pretrained=False,\n",
    "        num_classes=len(cid_to_spid),\n",
    "        checkpoint_path=pretrained_path,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    img = None\n",
    "    if \"https://\" in image_url or \"http://\" in image_url:\n",
    "        img = Image.open(urlopen(image_url))\n",
    "    elif image_url != None:\n",
    "        img = Image.open(image_url)\n",
    "\n",
    "    if img != None:\n",
    "        img = transforms(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)  # unsqueeze single image into batch of 1\n",
    "        top5_probabilities, top5_class_indices = torch.topk(\n",
    "            output.softmax(dim=1) * 100, k=5\n",
    "        )\n",
    "        top5_probabilities = top5_probabilities.cpu().detach().numpy()\n",
    "        top5_class_indices = top5_class_indices.cpu().detach().numpy()\n",
    "\n",
    "        for proba, cid in zip(top5_probabilities[0], top5_class_indices[0]):\n",
    "            species_id = cid_to_spid[cid]\n",
    "            species = spid_to_sp[species_id]\n",
    "            print(species_id, species, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths and image URL directly\n",
    "path = \"/mnt/data/models/pretrained_models\"\n",
    "image_url = \"https://lab.plantnet.org/LifeCLEF/PlantCLEF2024/single_plant_training_data/PlantCLEF2024singleplanttrainingdata/test/1361687/bd2d3830ac3270218ba82fd24e2290becd01317c.jpg\"\n",
    "class_mapping_file = f\"{path}/class_mapping.txt\"\n",
    "species_mapping_file = f\"{path}/species_id_to_name.txt\"\n",
    "model_path = \"vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all\"\n",
    "pretrained_path = f\"{path}/{model_path}/model_best.pth.tar\"\n",
    "\n",
    "main(image_url, class_mapping_file, species_mapping_file, pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/09 20:42:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/09 20:42:50 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f990c15dd80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "|                                       path|              image_name|                                              data|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "| /PlantCLEF2024test/CBN-Pla-B4-20160728.jpg| CBN-Pla-B4-20160728.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D3-20130808.jpg| CBN-Pla-D3-20130808.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "|/PlantCLEF2024test/CBN-PdlC-E4-20150701.jpg|CBN-PdlC-E4-20150701.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-F5-20150901.jpg| CBN-Pla-F5-20150901.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D1-20180724.jpg| CBN-Pla-D1-20180724.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get dataframes\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "test_data_path = \"data/parquet_files/PlantCLEF2024_test\"\n",
    "\n",
    "# paths to dataframe\n",
    "test_path = f\"{gcs_path}/{test_data_path}\"\n",
    "# read data\n",
    "test_df = spark.read.parquet(test_path)\n",
    "# show\n",
    "test_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, MapType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from plantclef.model_setup import setup_pretrained_model\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import vector_to_array, predict_batch_udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedDinoV2(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path: str,\n",
    "        input_col: str = \"input\",\n",
    "        output_col: str = \"output\",\n",
    "        model_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(inputCol=input_col, outputCol=output_col)\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.num_classes = 7806  # total number of plant species\n",
    "        self.local_directory = \"/mnt/data/models/pretrained_models\"\n",
    "        self.class_mapping_file = f\"{self.local_directory}/class_mapping.txt\"\n",
    "        # Model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = timm.create_model(\n",
    "            self.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=self.num_classes,\n",
    "            checkpoint_path=self.pretrained_path,\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # Data transform\n",
    "        self.data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        self.transforms = timm.data.create_transform(\n",
    "            **self.data_config, is_training=False\n",
    "        )\n",
    "        self.sql_statement = \"SELECT image_name, dino_logits FROM __THIS__\"\n",
    "\n",
    "    def _load_class_mapping(self):\n",
    "        with open(self.class_mapping_file) as f:\n",
    "            class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "        return class_index_to_class_name\n",
    "\n",
    "    def _make_predict_fn(self):\n",
    "        \"\"\"Return PredictBatchFunction using a closure over the model\"\"\"\n",
    "        self.cid_to_spid = self._load_class_mapping()\n",
    "\n",
    "        def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "            images = [Image.open(io.BytesIO(inputs)) for input in inputs]\n",
    "            processed_image = self.transforms(images).unsqueeze(0)\n",
    "            batch_input = torch.cat([processed_image]).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_input)\n",
    "                probabilities = torch.softmax(outputs, dim=1) * 100\n",
    "                top_probs, top_indices = torch.topk(probabilities, k=20)\n",
    "\n",
    "            top_probs = top_probs.cpu().numpy()\n",
    "            top_indices = top_indices.cpu().numpy()\n",
    "\n",
    "            # Convert top indices and probabilities to a dictionary\n",
    "            batch_results = []\n",
    "            result = {\n",
    "                self.cid_to_spid.get(index, \"Unknown\"): float(prob)\n",
    "                for index, prob in zip(top_indices.flatten(), top_probs.flatten())\n",
    "            }\n",
    "            batch_results.append(result)\n",
    "\n",
    "            return pd.Series(batch_results)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        return df.withColumn(\n",
    "            self.getOutputCol(),\n",
    "            predict_batch_udf(\n",
    "                make_predict_fn=self._make_predict_fn,\n",
    "                return_type=ArrayType(FloatType()),\n",
    "                batch_size=self.batch_size,\n",
    "            )(self.getInputCol()),\n",
    "        )\n",
    "\n",
    "    def transform(self, df) -> DataFrame:\n",
    "        transformed = self._transform(df)\n",
    "\n",
    "        for c in self.feature_columns:\n",
    "            # check if the feature is a vector and convert it to an array\n",
    "            if \"array\" in transformed.schema[c].simpleString():\n",
    "                continue\n",
    "            transformed = transformed.withColumn(c, vector_to_array(F.col(c)))\n",
    "        return transformed\n",
    "\n",
    "    @property\n",
    "    def feature_columns(self) -> list:\n",
    "        return [\"dino_logits\"]\n",
    "\n",
    "    def pipeline(self):\n",
    "        return Pipeline(stages=[self, SQLTransformer(statement=self.sql_statement)])\n",
    "\n",
    "    def run(self, df: DataFrame) -> DataFrame:\n",
    "        model = self.pipeline().fit(df)\n",
    "        transformed = model.transform(df)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = setup_pretrained_model()\n",
    "pretrained_dino = PretrainedDinoV2(\n",
    "    pretrained_path=pretrained_path,\n",
    "    input_col=\"data\",\n",
    "    output_col=\"dino_logits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 21:47:14 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 806, in predict\n",
      "    preds = predict_fn(single_input)\n",
      "  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in predict\n",
      "  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in <listcomp>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3309, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7a6649b313a0>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/09 21:47:14 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6) (plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 806, in predict\n",
      "    preds = predict_fn(single_input)\n",
      "  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in predict\n",
      "  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in <listcomp>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3309, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7a6649b313a0>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/09 21:47:14 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 806, in predict\n    preds = predict_fn(single_input)\n  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in predict\n  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in <listcomp>\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3309, in open\n    raise UnidentifiedImageError(msg)\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7a6649b313a0>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m pretrained_dino\u001b[38;5;241m.\u001b[39mrun(df\u001b[38;5;241m=\u001b[39mtest_df)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 806, in predict\n    preds = predict_fn(single_input)\n  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in predict\n  File \"/tmp/ipykernel_26242/2429910185.py\", line 51, in <listcomp>\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3309, in open\n    raise UnidentifiedImageError(msg)\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7a6649b313a0>\n"
     ]
    }
   ],
   "source": [
    "transformed_df = pretrained_dino.run(df=test_df)\n",
    "transformed_df.show(n=5, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
