{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DinoV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make directory to store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p /mnt/data/models\n",
    "# %cd /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the file from GCS to `/mnt/data/models` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp gs://dsgt-clef-plantclef-2024/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar /mnt/data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the `.tar` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xvf /mnt/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar -C /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use pretrained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=\";\", quoting=1, dtype={\"species_id\": str})\n",
    "    df = df.set_index(\"species_id\")\n",
    "    return df[\"species\"].to_dict()\n",
    "\n",
    "\n",
    "def main(image_url, class_mapping, species_mapping, pretrained_path):\n",
    "    cid_to_spid = load_class_mapping(class_mapping)\n",
    "    spid_to_sp = load_species_mapping(species_mapping)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = timm.create_model(\n",
    "        \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        pretrained=False,\n",
    "        num_classes=len(cid_to_spid),\n",
    "        checkpoint_path=pretrained_path,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    img = None\n",
    "    if \"https://\" in image_url or \"http://\" in image_url:\n",
    "        img = Image.open(urlopen(image_url))\n",
    "    elif image_url != None:\n",
    "        img = Image.open(image_url)\n",
    "\n",
    "    if img != None:\n",
    "        img = transforms(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)  # unsqueeze single image into batch of 1\n",
    "        top5_probabilities, top5_class_indices = torch.topk(\n",
    "            output.softmax(dim=1) * 100, k=5\n",
    "        )\n",
    "        top5_probabilities = top5_probabilities.cpu().detach().numpy()\n",
    "        top5_class_indices = top5_class_indices.cpu().detach().numpy()\n",
    "\n",
    "        for proba, cid in zip(top5_probabilities[0], top5_class_indices[0]):\n",
    "            species_id = cid_to_spid[cid]\n",
    "            species = spid_to_sp[species_id]\n",
    "            print(species_id, species, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths and image URL directly\n",
    "path = \"/mnt/data/models/pretrained_models\"\n",
    "image_url = \"https://lab.plantnet.org/LifeCLEF/PlantCLEF2024/single_plant_training_data/PlantCLEF2024singleplanttrainingdata/test/1361687/bd2d3830ac3270218ba82fd24e2290becd01317c.jpg\"\n",
    "class_mapping_file = f\"{path}/class_mapping.txt\"\n",
    "species_mapping_file = f\"{path}/species_id_to_name.txt\"\n",
    "model_path = \"vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all\"\n",
    "pretrained_path = f\"{path}/{model_path}/model_best.pth.tar\"\n",
    "\n",
    "main(image_url, class_mapping_file, species_mapping_file, pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 20:26:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/11 20:26:12 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x797109cf6d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "|                                       path|              image_name|                                              data|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "| /PlantCLEF2024test/CBN-Pla-B4-20160728.jpg| CBN-Pla-B4-20160728.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D3-20130808.jpg| CBN-Pla-D3-20130808.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "|/PlantCLEF2024test/CBN-PdlC-E4-20150701.jpg|CBN-PdlC-E4-20150701.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-F5-20150901.jpg| CBN-Pla-F5-20150901.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "| /PlantCLEF2024test/CBN-Pla-D1-20180724.jpg| CBN-Pla-D1-20180724.jpg|[FF D8 FF E0 00 10 4A 46 49 46 00 01 01 01 00 4...|\n",
      "+-------------------------------------------+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get dataframes\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "test_data_path = \"data/parquet_files/PlantCLEF2024_test\"\n",
    "\n",
    "# paths to dataframe\n",
    "test_path = f\"{gcs_path}/{test_data_path}\"\n",
    "# read data\n",
    "test_df = spark.read.parquet(test_path)\n",
    "# show\n",
    "test_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df = test_df.limit(10).cache()\n",
    "limit_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, MapType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from plantclef.model_setup import setup_pretrained_model\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedDinoV2(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path: str,\n",
    "        input_col: str = \"input\",\n",
    "        output_col: str = \"output\",\n",
    "        model_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(inputCol=input_col, outputCol=output_col)\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.num_classes = 7806  # total number of plant species\n",
    "        self.local_directory = \"/mnt/data/models/pretrained_models\"\n",
    "        self.class_mapping_file = f\"{self.local_directory}/class_mapping.txt\"\n",
    "        # Model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = timm.create_model(\n",
    "            self.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=self.num_classes,\n",
    "            checkpoint_path=self.pretrained_path,\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # Data transform\n",
    "        self.data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        self.transforms = timm.data.create_transform(\n",
    "            **self.data_config, is_training=False\n",
    "        )\n",
    "        self.sql_statement = \"SELECT image_name, dino_logits FROM __THIS__\"\n",
    "        self.cid_to_spid = self._load_class_mapping()\n",
    "\n",
    "    def _load_class_mapping(self):\n",
    "        with open(self.class_mapping_file) as f:\n",
    "            class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "        return class_index_to_class_name\n",
    "\n",
    "    def _make_predict_fn(self):\n",
    "        \"\"\"Return PredictBatchFunction using a closure over the model\"\"\"\n",
    "\n",
    "        def predict(input_data: list) -> list:\n",
    "            # Load all inputs into a batch of images\n",
    "            img = Image.open(io.BytesIO(input_data))\n",
    "            # Transform and stack images to a single tensor\n",
    "            processed_image = self.transforms(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(processed_image)\n",
    "                probabilities = torch.softmax(outputs, dim=1) * 100\n",
    "                top_probs, top_indices = torch.topk(probabilities, k=20)\n",
    "\n",
    "            top_probs = top_probs.cpu().numpy()[0]\n",
    "            top_indices = top_indices.cpu().numpy()[0]\n",
    "\n",
    "            # Convert top indices and probabilities to a dictionary\n",
    "            result = [\n",
    "                {self.cid_to_spid.get(index, \"Unknown\"): float(prob)}\n",
    "                for index, prob in zip(top_indices, top_probs)\n",
    "            ]\n",
    "            return result\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Create a UDF from the predict function\n",
    "        predict_fn = self._make_predict_fn()\n",
    "        predict_udf = F.udf(predict_fn, ArrayType(MapType(StringType(), FloatType())))\n",
    "\n",
    "        return df.withColumn(\n",
    "            self.getOutputCol(),\n",
    "            predict_udf(self.getInputCol()),\n",
    "        )\n",
    "\n",
    "    def transform(self, df) -> DataFrame:\n",
    "        transformed = self._transform(df)\n",
    "\n",
    "        for c in self.feature_columns:\n",
    "            # check if the feature is a vector and convert it to an array\n",
    "            if \"array\" in transformed.schema[c].simpleString():\n",
    "                continue\n",
    "            transformed = transformed.withColumn(c, vector_to_array(F.col(c)))\n",
    "        return transformed\n",
    "\n",
    "    @property\n",
    "    def feature_columns(self) -> list:\n",
    "        return [\"dino_logits\"]\n",
    "\n",
    "    def pipeline(self):\n",
    "        return Pipeline(stages=[self, SQLTransformer(statement=self.sql_statement)])\n",
    "\n",
    "    def run(self, df: DataFrame) -> DataFrame:\n",
    "        model = self.pipeline().fit(df)\n",
    "        transformed = model.transform(df)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = setup_pretrained_model()\n",
    "pretrained_dino = PretrainedDinoV2(\n",
    "    pretrained_path=pretrained_path,\n",
    "    input_col=\"data\",\n",
    "    output_col=\"dino_logits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------------------------+\n",
      "|              image_name|                                       dino_logits|\n",
      "+------------------------+--------------------------------------------------+\n",
      "| CBN-Pla-B4-20160728.jpg|[{1361281 -> 7.84536}, {1741880 -> 4.1319184}, ...|\n",
      "| CBN-Pla-D3-20130808.jpg|[{1741706 -> 6.7095423}, {1741903 -> 5.509813},...|\n",
      "|CBN-PdlC-E4-20150701.jpg|[{1392323 -> 6.9817915}, {1361281 -> 5.2335153}...|\n",
      "| CBN-Pla-F5-20150901.jpg|[{1361281 -> 14.984538}, {1390764 -> 10.862804}...|\n",
      "| CBN-Pla-D1-20180724.jpg|[{1361281 -> 35.631886}, {1651363 -> 21.19073},...|\n",
      "+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df = pretrained_dino.run(df=limit_df).cache()\n",
    "transformed_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- dino_logits: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: float (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "class PretrainedInferenceTask:\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_root_dir: str,\n",
    "        k: int = 5,\n",
    "    ):\n",
    "        self.default_root_dir = default_root_dir\n",
    "        self.k = k\n",
    "\n",
    "    def _format_species_ids(self, species_ids: list) -> str:\n",
    "        \"\"\"Formats the species IDs in single square brackets, separated by commas.\"\"\"\n",
    "        formatted_ids = \", \".join(str(id) for id in species_ids)\n",
    "        return f\"[{formatted_ids}]\"\n",
    "\n",
    "    def _extract_top_k_species(self, logits: list) -> list:\n",
    "        \"\"\"Extracts the top k species from the logits list.\"\"\"\n",
    "        top_logits = [list(item.keys())[0] for item in logits[: self.k]]\n",
    "        set_logits = sorted(set(top_logits), key=top_logits.index)\n",
    "        return set_logits\n",
    "\n",
    "    def _remove_extension(self, filename: str) -> str:\n",
    "        \"\"\"Removes the file extension from the filename.\"\"\"\n",
    "        return filename.rsplit(\".\", 1)[0]\n",
    "\n",
    "    def _prepare_and_write_submission(self, spark_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Converts Spark DataFrame to Pandas, formats it, and writes to GCS.\"\"\"\n",
    "        records = []\n",
    "        for row in spark_df.collect():\n",
    "            image_name = self._remove_extension(row[\"image_name\"])\n",
    "            logits = row[\"dino_logits\"]\n",
    "            top_k_species = self._extract_top_k_species(logits)\n",
    "            formatted_species = self._format_species_ids(top_k_species)\n",
    "            records.append({\"plot_id\": image_name, \"species_ids\": formatted_species})\n",
    "\n",
    "        pandas_df = pd.DataFrame(records)\n",
    "        return pandas_df\n",
    "\n",
    "    def write_submission_csv(self, df, output_path):\n",
    "        \"\"\"Writes the predictions to a CSV file in the specified format.\"\"\"\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(\n",
    "                file, delimiter=\";\", quoting=csv.QUOTE_NONE, escapechar=\"\\\\\"\n",
    "            )\n",
    "            writer.writerow([\"plot_id\", \"species_ids\"])\n",
    "\n",
    "            df = df.orderBy(\"image_name\")\n",
    "            for row in df.collect():\n",
    "                image_name = self._remove_extension(row[\"image_name\"])\n",
    "                logits = row[\"dino_logits\"]\n",
    "                top_k_species = self._extract_top_k_species(logits)\n",
    "                formatted_species = self._format_species_ids(top_k_species)\n",
    "                writer.writerow([image_name, formatted_species])\n",
    "\n",
    "    def _write_csv_to_gcs(self, df):\n",
    "        \"\"\"Writes the Pandas DataFrame to a CSV file in GCS.\"\"\"\n",
    "        output_path = f\"{self.default_root_dir}/dsgt_run_top_{self.k}_species.csv\"\n",
    "        # Use csv module directly to control quoting more granularly\n",
    "        with open(output_path, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(\n",
    "                file, delimiter=\";\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            writer.writerow([\"plot_id\", \"species_ids\"])\n",
    "            for index, row in df.iterrows():\n",
    "                writer.writerow([row[\"plot_id\"], row[\"species_ids\"]])\n",
    "\n",
    "    def run(self, transformed_df: DataFrame):\n",
    "        pandas_df = self._prepare_and_write_submission(transformed_df)\n",
    "        self._write_csv_to_gcs(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_root_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "model_dir_path = \"models/pretrained-dino\"\n",
    "default_root_dir = f\"{gcs_root_path}/{model_dir_path}\"\n",
    "\n",
    "inference = PretrainedInferenceTask(default_root_dir, k=5)\n",
    "# inference.run(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inference = PretrainedInferenceTask(default_root_dir, k=5)\n",
    "inference.write_submission_csv(transformed_df, \"dsgt_run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------------------------+\n",
      "|              image_name|                                       dino_logits|\n",
      "+------------------------+--------------------------------------------------+\n",
      "|CBN-PdlC-D4-20130903.jpg|[{1395807 -> 13.998325}, {1741880 -> 10.6929455...|\n",
      "|CBN-PdlC-A3-20200722.jpg|[{1412857 -> 28.893694}, {1742052 -> 2.8326354}...|\n",
      "| CBN-Pla-B4-20150723.jpg|[{1361281 -> 22.123766}, {1741880 -> 4.236441},...|\n",
      "|CBN-PdlC-D5-20190722.jpg|[{1392732 -> 17.866493}, {1392323 -> 11.805281}...|\n",
      "| CBN-Pla-E6-20190723.jpg|[{1397070 -> 10.023462}, {1361043 -> 2.383624},...|\n",
      "+------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get dataframes\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "data_path = \"data/process/pretrained_dino/dino_pretrained/data\"\n",
    "\n",
    "# paths to dataframe\n",
    "df_path = f\"{gcs_path}/{data_path}\"\n",
    "# read data\n",
    "pretrained_df = spark.read.parquet(df_path)\n",
    "# show\n",
    "pretrained_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inference = PretrainedInferenceTask(default_root_dir, k=5)\n",
    "inference.write_submission_csv(pretrained_df, \"dsgt_run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
