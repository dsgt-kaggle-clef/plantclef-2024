{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DinoV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make directory to store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# ! mkdir -p /mnt/data/models\n",
    "# %cd /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the file from GCS to `/mnt/data/models` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp gs://dsgt-clef-plantclef-2024/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar /mnt/data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the `.tar` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xvf /mnt/data/models/PlantNet_PlantCLEF2024_pretrained_models_on_the_flora_of_south-western_europe.tar -C /mnt/data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use pretrained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=\";\", quoting=1, dtype={\"species_id\": str})\n",
    "    df = df.set_index(\"species_id\")\n",
    "    return df[\"species\"].to_dict()\n",
    "\n",
    "\n",
    "def main(image_url, class_mapping, species_mapping, pretrained_path):\n",
    "    cid_to_spid = load_class_mapping(class_mapping)\n",
    "    spid_to_sp = load_species_mapping(species_mapping)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = timm.create_model(\n",
    "        \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        pretrained=False,\n",
    "        num_classes=len(cid_to_spid),\n",
    "        checkpoint_path=pretrained_path,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    # get model specific transforms (normalization, resize)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "    img = None\n",
    "    if \"https://\" in image_url or \"http://\" in image_url:\n",
    "        img = Image.open(urlopen(image_url))\n",
    "    elif image_url != None:\n",
    "        img = Image.open(image_url)\n",
    "\n",
    "    if img != None:\n",
    "        img = transforms(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        output = model(img)  # unsqueeze single image into batch of 1\n",
    "        top5_probabilities, top5_class_indices = torch.topk(\n",
    "            output.softmax(dim=1) * 100, k=5\n",
    "        )\n",
    "        top5_probabilities = top5_probabilities.cpu().detach().numpy()\n",
    "        top5_class_indices = top5_class_indices.cpu().detach().numpy()\n",
    "\n",
    "        for proba, cid in zip(top5_probabilities[0], top5_class_indices[0]):\n",
    "            species_id = cid_to_spid[cid]\n",
    "            species = spid_to_sp[species_id]\n",
    "            print(species_id, species, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths and image URL directly\n",
    "path = \"/mnt/data/models/pretrained_models\"\n",
    "image_url = \"https://lab.plantnet.org/LifeCLEF/PlantCLEF2024/single_plant_training_data/PlantCLEF2024singleplanttrainingdata/test/1361687/bd2d3830ac3270218ba82fd24e2290becd01317c.jpg\"\n",
    "class_mapping_file = f\"{path}/class_mapping.txt\"\n",
    "species_mapping_file = f\"{path}/species_id_to_name.txt\"\n",
    "model_path = \"vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all\"\n",
    "pretrained_path = f\"{path}/{model_path}/model_best.pth.tar\"\n",
    "\n",
    "main(image_url, class_mapping_file, species_mapping_file, pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframes\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024\"\n",
    "test_data_path = \"data/parquet_files/PlantCLEF2024_test\"\n",
    "\n",
    "# paths to dataframe\n",
    "test_path = f\"{gcs_path}/{test_data_path}\"\n",
    "# read data\n",
    "test_df = spark.read.parquet(test_path)\n",
    "# show\n",
    "test_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, MapType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from plantclef.model_setup import setup_pretrained_model\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedDinoV2(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path: str,\n",
    "        input_col: str = \"input\",\n",
    "        output_col: str = \"output\",\n",
    "        model_name: str = \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._setDefault(inputCol=input_col, outputCol=output_col)\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.num_classes = 7806  # total number of plant species\n",
    "        self.local_directory = \"/mnt/data/models/pretrained_models\"\n",
    "        self.class_mapping_file = f\"{self.local_directory}/class_mapping.txt\"\n",
    "        # Model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = timm.create_model(\n",
    "            self.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=self.num_classes,\n",
    "            checkpoint_path=self.pretrained_path,\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # Data transform\n",
    "        self.data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        self.transforms = timm.data.create_transform(\n",
    "            **self.data_config, is_training=False\n",
    "        )\n",
    "        self.sql_statement = \"SELECT image_name, dino_logits FROM __THIS__\"\n",
    "\n",
    "    def _load_class_mapping(self):\n",
    "        with open(self.class_mapping_file) as f:\n",
    "            class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "        return class_index_to_class_name\n",
    "\n",
    "    def _make_predict_fn(self):\n",
    "        \"\"\"Return PredictBatchFunction using a closure over the model\"\"\"\n",
    "        self.cid_to_spid = self._load_class_mapping()\n",
    "\n",
    "        def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "            batch_results = []\n",
    "            for i, input in enumerate(inputs):\n",
    "                print(f\"Item {i} type: {type(input)}\")  # Check the type of the input\n",
    "                if not isinstance(input, bytes):\n",
    "                    print(\"Error: Input is not bytes.\")\n",
    "                    batch_results.append({})\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    image = Image.open(io.BytesIO(input))\n",
    "                    processed_image = self.transforms(image).unsqueeze(0)\n",
    "                    batch_input = torch.cat([processed_image]).to(self.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(batch_input)\n",
    "                        probabilities = torch.softmax(outputs, dim=1) * 100\n",
    "                        top_probs, top_indices = torch.topk(probabilities, k=20)\n",
    "\n",
    "                    top_probs = top_probs.cpu().numpy()\n",
    "                    top_indices = top_indices.cpu().numpy()\n",
    "\n",
    "                    # Convert top indices and probabilities to a dictionary\n",
    "                    result = {\n",
    "                        self.cid_to_spid.get(index, \"Unknown\"): float(prob)\n",
    "                        for index, prob in zip(\n",
    "                            top_indices.flatten(), top_probs.flatten()\n",
    "                        )\n",
    "                    }\n",
    "                    batch_results.append(result)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process input due to: {str(e)}\")\n",
    "                    batch_results.append({})\n",
    "\n",
    "            return pd.Series(batch_results)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def _transform(self, df):\n",
    "        print(f\"df schema: {df.schema}\")\n",
    "        predict_udf = F.udf(\n",
    "            self._make_predict_fn(), ArrayType(MapType(StringType(), FloatType()))\n",
    "        )\n",
    "        return df.withColumn(self.getOutputCol(), predict_udf(df[self.getInputCol()]))\n",
    "\n",
    "    def transform(self, df) -> DataFrame:\n",
    "        transformed = self._transform(df)\n",
    "\n",
    "        for c in self.feature_columns:\n",
    "            # check if the feature is a vector and convert it to an array\n",
    "            if \"array\" in transformed.schema[c].simpleString():\n",
    "                continue\n",
    "            transformed = transformed.withColumn(c, vector_to_array(F.col(c)))\n",
    "        return transformed\n",
    "\n",
    "    @property\n",
    "    def feature_columns(self) -> list:\n",
    "        return [\"dino_logits\"]\n",
    "\n",
    "    def pipeline(self):\n",
    "        return Pipeline(stages=[self, SQLTransformer(statement=self.sql_statement)])\n",
    "\n",
    "    def run(self, df: DataFrame) -> DataFrame:\n",
    "        model = self.pipeline().fit(df)\n",
    "        transformed = model.transform(df)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = setup_pretrained_model()\n",
    "pretrained_dino = PretrainedDinoV2(\n",
    "    pretrained_path=pretrained_path,\n",
    "    input_col=\"data\",\n",
    "    output_col=\"dino_logits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pretrained_dino.run(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
