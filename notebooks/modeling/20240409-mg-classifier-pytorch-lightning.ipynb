{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with pyspark and pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/process\"\n",
    "dct_emb_train = \"training_cropped_resized_v2/dino_dct/data\"\n",
    "\n",
    "# Define the GCS path to the embedding files\n",
    "dct_gcs_path = f\"{gcs_path}/{dct_emb_train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "dct_df = spark.read.parquet(dct_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "dct_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data, num_classes):\n",
    "        self.data = data.toPandas()  # Convert to Pandas DF\n",
    "        self.num_classes = num_classes  # Total number of classes\n",
    "        self.species_id = self._get_species_index(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_species_index(self, data):\n",
    "        species_ids = (\n",
    "            data.select(\"species_id\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "        )\n",
    "        species_id_to_index = {\n",
    "            species_id: idx for idx, species_id in enumerate(species_ids)\n",
    "        }\n",
    "        return species_id_to_index\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        embeddings = torch.tensor(row[\"dct_embedding\"])\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "        species_index = self.species_id[row[\"species_id\"]]\n",
    "        labels[species_index] = 1.0\n",
    "        return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Recall, HammingLoss\n",
    "\n",
    "\n",
    "class MultiLabelClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(num_features, num_classes)\n",
    "        self.accuracy = Accuracy(threshold=0.5, average=\"samples\", multilabel=True)\n",
    "        self.f1_score = F1Score(\n",
    "            threshold=0.5, average=\"samples\", num_classes=num_classes\n",
    "        )\n",
    "        self.precision = Precision(\n",
    "            threshold=0.5, avereage=\"samples\", num_classes=num_classes\n",
    "        )\n",
    "        self.recall = Recall(threshold=0.5, average=\"samples\", num_classes=num_classes)\n",
    "        self.hamming_loss = HammingLoss(threshold=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(\n",
    "            self.layer(x)\n",
    "        )  # Using sigmoid for multi-label classification\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.binary_cross_entropy(\n",
    "            y_hat, y.float()\n",
    "        )  # Ensure y is float for BCE\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # Update metrics\n",
    "        self.log(\"val_accuracy\", self.accuracy(y_hat, y))\n",
    "        self.log(\"val_f1\", self.f1_score(y_hat, y))\n",
    "        self.log(\"val_precision\", self.precision(y_hat, y))\n",
    "        self.log(\"val_recall\", self.recall(y_hat, y))\n",
    "        self.log(\"val_hamming_loss\", self.hamming_loss(y_hat, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get a small subset of the dataset\n",
    "sub_species_df = (\n",
    "    dct_df.groupBy(\"species_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .where(F.col(\"n\") > 100)\n",
    "    .orderBy(F.rand(seed=42))\n",
    "    .limit(10)\n",
    ").cache()\n",
    "sub_species_df.show(truncate=80)\n",
    "\n",
    "# Collect the species_id into a list of values\n",
    "species_id_subset = [\n",
    "    row[\"species_id\"]\n",
    "    for row in sub_species_df.select(\"species_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "# Get subset of images to test pipeline\n",
    "subset_df = dct_df.where(F.col(\"species_id\").isin(species_id_subset)).cache()\n",
    "subset_df.show()\n",
    "\n",
    "# Count number of rows in subset_df\n",
    "print(f\"subset_df count: {subset_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train-validation split\n",
    "train_df, valid_df = subset_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache the splits to improve performance\n",
    "train_df = train_df.cache()\n",
    "valid_df = valid_df.cache()\n",
    "\n",
    "print(f\"Train DF count: {train_df.count()}\")\n",
    "print(f\"Valid DF count: {valid_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init params\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Prepare PyTorch datasets\n",
    "train_data = EmbeddingDataset(data=train_df, num_classes=num_classes)\n",
    "valid_data = EmbeddingDataset(data=valid_df, num_classes=num_classes)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model\n",
    "model = MultiLabelClassifier(\n",
    "    num_features=64, num_classes=num_classes\n",
    ")  # Only using 10 classses for testing\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=epochs)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
