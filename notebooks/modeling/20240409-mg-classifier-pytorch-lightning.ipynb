{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with pyspark and pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/14 17:50:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/14 17:50:32 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://big-disk-dev.us-central1-b.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c16ac966230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "|                                  image_name|species_id|                                     dct_embedding|\n",
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "|170e88ca9af457daa1038092479b251c61c64f7d.jpg|   1742956|[-20648.51, 2133.689, -2555.3125, 14820.57, 685...|\n",
      "|c24a2d8646f5bc7112a39908bd2f6c45bf066a71.jpg|   1356834|[-25395.82, -12564.387, 24736.02, 20483.8, 2115...|\n",
      "|e1f68e5f05618921969aee2575de20e537e6d66b.jpg|   1563754|[-26178.633, -7670.404, -22552.29, -6563.006, 8...|\n",
      "|b0433cd6968b57d52e5c25dc45a28e674a25e61e.jpg|   1367432|[-23662.764, -6773.8213, -8283.518, 3769.6064, ...|\n",
      "|96478a0fe20a41e755b0c8d798690f2c2b7c115f.jpg|   1389010|[-22182.172, -19444.006, 23355.23, 7042.8604, -...|\n",
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/process\"\n",
    "dct_emb_train = \"training_cropped_resized_v2/dino_dct/data\"\n",
    "\n",
    "# Define the GCS path to the embedding files\n",
    "dct_gcs_path = f\"{gcs_path}/{dct_emb_train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "dct_df = spark.read.parquet(dct_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "dct_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, spark_df):\n",
    "        self.embeddings = [x[\"dct_embedding\"] for x in spark_df.collect()]\n",
    "        self.labels = [x[\"index\"] for x in spark_df.collect()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embeddings = torch.tensor(self.embeddings[index], dtype=torch.float)\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return embeddings, labels\n",
    "\n",
    "\n",
    "class MultiClassClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # Saves hyperparams in the checkpoints\n",
    "        self.layer = nn.Linear(num_features, num_classes)\n",
    "        self.learning_rate = 0.002\n",
    "        self.accuracy = MulticlassAccuracy(num_classes=num_classes, average=\"weighted\")\n",
    "        self.f1_score = MulticlassF1Score(num_classes=num_classes, average=\"weighted\")\n",
    "        self.precision = MulticlassPrecision(\n",
    "            num_classes=num_classes, average=\"weighted\"\n",
    "        )\n",
    "        self.recall = MulticlassRecall(num_classes=num_classes, average=\"weighted\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.layer(x), dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # Use negative log likelihood loss for multiclass classification\n",
    "        loss = torch.nn.functional.nll_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        valid_loss = {\"valid_loss\": torch.nn.functional.nll_loss(y_hat, y)}\n",
    "        # Update metrics\n",
    "        self.log(\n",
    "            \"valid_accuracy\", self.accuracy(y_hat, y), on_step=False, on_epoch=True\n",
    "        )\n",
    "        self.log(\"valid_f1\", self.f1_score(y_hat, y), on_step=False, on_epoch=True)\n",
    "        self.log(\n",
    "            \"valid_precision\", self.precision(y_hat, y), on_step=False, on_epoch=True\n",
    "        )\n",
    "        self.log(\"valid_recall\", self.recall(y_hat, y), on_step=False, on_epoch=True)\n",
    "        return valid_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare subset of data for testing end-to-end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Set variable to limit number of species in subset data\n",
    "LIMIT_SPECIES = 5\n",
    "\n",
    "# Get a small subset of the dataset\n",
    "sub_species_df = (\n",
    "    dct_df.groupBy(\"species_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .where(F.col(\"n\") > 100)\n",
    "    .orderBy(F.rand(seed=42))\n",
    "    .limit(LIMIT_SPECIES)\n",
    ")\n",
    "\n",
    "# Collect the species_id into a list of values\n",
    "species_id_subset = [\n",
    "    row[\"species_id\"]\n",
    "    for row in sub_species_df.select(\"species_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "# Assign an idex to each species_id\n",
    "sub_species_idx_df = (\n",
    "    sub_species_df.select(\"species_id\")\n",
    "    .distinct()\n",
    "    .withColumn(\"index\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# Get subset of images to test pipeline\n",
    "subset_df = (\n",
    "    dct_df.join(F.broadcast(sub_species_idx_df), \"species_id\", \"inner\")\n",
    "    .drop(\"n\")\n",
    "    .where(F.col(\"species_id\").isin(species_id_subset))\n",
    ")\n",
    "\n",
    "# # Show DFs and count number of rows in subset_df\n",
    "# sub_species_df.show(truncate=80)\n",
    "# subset_df.show()\n",
    "# print(f\"subset_df count: {subset_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final subset of data, species with species having >= 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped DF count: 4797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DCT DF count: 1323820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----+\n",
      "|species_id|          image_name|       dct_embedding|index|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "|   1742956|170e88ca9af457daa...|[-20648.51, 2133....| 3769|\n",
      "|   1367432|b0433cd6968b57d52...|[-23662.764, -677...| 4656|\n",
      "|   1360260|2b0301802884c465a...|[-26661.809, 1455...| 2669|\n",
      "|   1378563|e60616fddca9656d3...|[-31823.447, 393....| 1494|\n",
      "|   1358543|24c155bbe8f0e77ae...|[-21757.35, 2965....| 2808|\n",
      "|   1390691|c469f9a672a9b1d96...|[-27213.7, 20937....|  795|\n",
      "|   1361302|39092f483d3fdef57...|[-22082.877, -312...| 4744|\n",
      "|   1356428|8b33e8cd6dad65d5f...|[-32586.871, 3672...|   23|\n",
      "|   1586076|6e393f50b7a31dea1...|[-28792.238, 1479...|  455|\n",
      "|   1360262|a6a5272e3310a299e...|[-28321.89, 11192...|  886|\n",
      "|   1362293|1fd06740a23e7353a...|[-20066.348, -968...| 1007|\n",
      "|   1363358|587b331459d6f4643...|[-26982.426, 2332...|   75|\n",
      "|   1390699|1f9ac6784cfae5012...|[-19362.758, -649...| 3654|\n",
      "|   1360978|b46920705c0de7ccd...|[-18360.469, 1367...| 4464|\n",
      "|   1361026|22cc5c05ac3fdbd80...|[-23249.34, 2917....|  144|\n",
      "|   1399742|7c725df0a027a3ffd...|[-23979.717, 3276...|  484|\n",
      "|   1486742|5b14d50dd43600b11...|[-21152.365, -247...| 4194|\n",
      "|   1364154|866460203a1add9df...|[-18820.883, 8223...| 3228|\n",
      "|   1360566|977331457e310fae4...|[-27539.668, 5645...| 4711|\n",
      "|   1397442|9fbdb46534c3c2cdb...|[-23249.508, -107...| 2445|\n",
      "+----------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Param\n",
    "SPECIES_IMAGE_COUNT = 100\n",
    "\n",
    "# Transformation\n",
    "grouped_df = (\n",
    "    dct_df.groupBy(\"species_id\")\n",
    "    .agg(F.count(\"species_id\").alias(\"n\"))\n",
    "    .filter(f\"n >= {SPECIES_IMAGE_COUNT}\")\n",
    "    .orderBy(F.col(\"n\").desc())\n",
    ")\n",
    "grouped_count = grouped_df.count()\n",
    "print(f\"Grouped DF count: {grouped_count}\")\n",
    "\n",
    "# Assign an idex to each species_id\n",
    "species_idx_df = (\n",
    "    grouped_df.select(\"species_id\")\n",
    "    .distinct()\n",
    "    .withColumn(\"index\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# Join the grouped_df with dct_df to get the selected species_id\n",
    "filtered_dct_df = dct_df.join(F.broadcast(species_idx_df), \"species_id\", \"inner\").drop(\n",
    "    \"n\"\n",
    ")\n",
    "filtered_dct_count = filtered_dct_df.count()\n",
    "print(f\"Filtered DCT DF count: {filtered_dct_count}\")\n",
    "\n",
    "# Show final DF\n",
    "filtered_dct_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 986, valid: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:==================================================>     (17 + 2) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform a train-validation split\n",
    "def train_valid_split(df):\n",
    "    train_df, valid_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "# Pass desired DF to function\n",
    "train_df, valid_df = train_valid_split(df=subset_df)\n",
    "print(f\"train: {train_df.count()}, valid: {valid_df.count()}\")\n",
    "\n",
    "# Init params\n",
    "NUM_CLASSES = int(train_df.select(\"species_id\").distinct().count())\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "print(f\"Num classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets\n",
    "train_dataset = EmbeddingDataset(spark_df=train_df)\n",
    "valid_dataset = EmbeddingDataset(spark_df=valid_df)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/mgustine/plantclef-2024/notebooks/modeling/lightning_logs\n",
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/mgustine/plantclef-2024/notebooks/modeling exists and is not empty.\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | layer     | Linear              | 325   \n",
      "1 | accuracy  | MulticlassAccuracy  | 0     \n",
      "2 | f1_score  | MulticlassF1Score   | 0     \n",
      "3 | precision | MulticlassPrecision | 0     \n",
      "4 | recall    | MulticlassRecall    | 0     \n",
      "--------------------------------------------------\n",
      "325       Trainable params\n",
      "0         Non-trainable params\n",
      "325       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 31/31 [00:00<00:00, 193.04it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 31/31 [00:00<00:00, 190.64it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "# Current path\n",
    "curr_dir = Path(os.getcwd())\n",
    "\n",
    "# Setup checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"valid_accuracy\",  # Monitor validation accuracy for improvement\n",
    "    dirpath=curr_dir,  # Directory path for saving checkpoints\n",
    "    filename=\"model-{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    save_top_k=1,  # Save only top 1 model\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "# model\n",
    "model = MultiClassClassifier(\n",
    "    num_features=64,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n",
      "{'num_features': 64, 'num_classes': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiClassClassifier(\n",
       "  (layer): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (accuracy): MulticlassAccuracy()\n",
       "  (f1_score): MulticlassF1Score()\n",
       "  (precision): MulticlassPrecision()\n",
       "  (recall): MulticlassRecall()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the best model is saved in the same directory as your script\n",
    "best_model = \"model-epoch=06-val_accuracy=0.00.ckpt\"\n",
    "checkpoint_path = curr_dir / Path(best_model)  # Adjust filename as necessary\n",
    "\n",
    "# Load the checkpoint file\n",
    "checkpoint = torch.load(\n",
    "    checkpoint_path, map_location=torch.device(\"cpu\")\n",
    ")  # Use 'cpu' to avoid GPU memory issues\n",
    "\n",
    "# Print the keys and any hyperparameters stored in the checkpoint\n",
    "print(checkpoint.keys())\n",
    "if \"hyper_parameters\" in checkpoint:\n",
    "    print(checkpoint[\"hyper_parameters\"])\n",
    "else:\n",
    "    print(\"No hyperparameters stored in this checkpoint.\")\n",
    "\n",
    "# Load the trained model from checkpoint\n",
    "model = MultiClassClassifier.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_features=64,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch  # Adjust these based on how your data is structured\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(\n",
    "                outputs.data, 1\n",
    "            )  # Get the index of the max log-probability\n",
    "            predictions.extend(predicted.tolist())\n",
    "            labels.extend(targets.tolist())\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# Call the function\n",
    "predictions, labels = validate_model(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:==================================================>     (17 + 2) / 19]\r"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "# Target names\n",
    "target_names = [\n",
    "    str(row[\"index\"]) for row in train_df.select(\"index\").distinct().collect()\n",
    "]\n",
    "report = classification_report(labels, predictions, target_names=target_names)\n",
    "\n",
    "# Print scores\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
