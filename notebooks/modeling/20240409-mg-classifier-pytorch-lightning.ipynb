{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with pyspark and pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/09 12:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/09 12:50:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://plantclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plantclef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b29d378f310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "|                                  image_name|species_id|                                     dct_embedding|\n",
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "|170e88ca9af457daa1038092479b251c61c64f7d.jpg|   1742956|[-20648.51, 2133.689, -2555.3125, 14820.57, 685...|\n",
      "|c24a2d8646f5bc7112a39908bd2f6c45bf066a71.jpg|   1356834|[-25395.82, -12564.387, 24736.02, 20483.8, 2115...|\n",
      "|e1f68e5f05618921969aee2575de20e537e6d66b.jpg|   1563754|[-26178.633, -7670.404, -22552.29, -6563.006, 8...|\n",
      "|b0433cd6968b57d52e5c25dc45a28e674a25e61e.jpg|   1367432|[-23662.764, -6773.8213, -8283.518, 3769.6064, ...|\n",
      "|96478a0fe20a41e755b0c8d798690f2c2b7c115f.jpg|   1389010|[-22182.172, -19444.006, 23355.23, 7042.8604, -...|\n",
      "+--------------------------------------------+----------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/process\"\n",
    "dct_emb_train = \"training_cropped_resized_v2/dino_dct/data\"\n",
    "\n",
    "# Define the GCS path to the embedding files\n",
    "dct_gcs_path = f\"{gcs_path}/{dct_emb_train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "dct_df = spark.read.parquet(dct_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "dct_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data, num_classes):\n",
    "        self.data = data.toPandas()  # Convert to Pandas DF\n",
    "        self.num_classes = num_classes  # Total number of classes\n",
    "        self.species_id = self._get_species_index(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_species_index(self, data):\n",
    "        species_ids = (\n",
    "            data.select(\"species_id\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "        )\n",
    "        species_id_to_index = {\n",
    "            species_id: idx for idx, species_id in enumerate(species_ids)\n",
    "        }\n",
    "        return species_id_to_index\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        embeddings = torch.tensor(row[\"dct_embedding\"])\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "        species_index = self.species_id[row[\"species_id\"]]\n",
    "        labels[species_index] = 1.0\n",
    "        return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelAccuracy,\n",
    "    MultilabelF1Score,\n",
    "    MultilabelPrecision,\n",
    "    MultilabelRecall,\n",
    ")\n",
    "\n",
    "\n",
    "class MultiLabelClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(num_features, num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(threshold=0.5, num_labels=num_classes)\n",
    "        self.f1_score = MultilabelF1Score(threshold=0.5, num_labels=num_classes)\n",
    "        self.precision = MultilabelPrecision(threshold=0.5, num_labels=num_classes)\n",
    "        self.recall = MultilabelRecall(threshold=0.5, num_labels=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(\n",
    "            self.layer(x)\n",
    "        )  # Using sigmoid for multi-label classification\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.binary_cross_entropy(\n",
    "            y_hat, y.float()\n",
    "        )  # Ensure y is float for BCE\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # Update metrics\n",
    "        self.log(\"val_accuracy\", self.accuracy(y_hat, y))\n",
    "        self.log(\"val_f1\", self.f1_score(y_hat, y))\n",
    "        self.log(\"val_precision\", self.precision(y_hat, y))\n",
    "        self.log(\"val_recall\", self.recall(y_hat, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 13:00:22 WARN CacheManager: Asked to cache already cached data.\n",
      "24/04/09 13:00:22 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|species_id|  n|\n",
      "+----------+---+\n",
      "|   1389268|291|\n",
      "|   1358860|247|\n",
      "|   1363409|271|\n",
      "|   1396750|190|\n",
      "|   1394732|243|\n",
      "|   1359227|196|\n",
      "|   1363259|262|\n",
      "|   1393571|264|\n",
      "|   1393681|248|\n",
      "|   1360715|223|\n",
      "+----------+---+\n",
      "\n",
      "+--------------------+----------+--------------------+\n",
      "|          image_name|species_id|       dct_embedding|\n",
      "+--------------------+----------+--------------------+\n",
      "|1417f00b385c9648e...|   1359227|[-21118.215, 1026...|\n",
      "|1796804389a7af364...|   1393571|[-17140.287, -229...|\n",
      "|f9bda2da2c8817243...|   1363409|[-18112.29, -9305...|\n",
      "|3512914e3568872bc...|   1389268|[-31069.455, 1593...|\n",
      "|a54095a70be70d3dd...|   1363259|[-23388.5, 1492.1...|\n",
      "|2fd167d70f666ef5a...|   1393571|[-22588.898, 3876...|\n",
      "|497c2b1590ed58e12...|   1393571|[-18909.512, -167...|\n",
      "|af24c93fa73a97312...|   1393681|[-16674.791, -137...|\n",
      "|5c88ac6797d82c04b...|   1363259|[-26085.584, 3116...|\n",
      "|67053d421a3b1ef9b...|   1396750|[-28502.367, 9868...|\n",
      "|37bba53333910dfab...|   1393571|[-29814.75, 8351....|\n",
      "|e35494a6e5e4ff5b6...|   1358860|[-32065.266, 1658...|\n",
      "|45c2ed90891d9ea87...|   1394732|[-29510.152, -118...|\n",
      "|7b09245382e5e3db4...|   1363409|[-13146.777, -226...|\n",
      "|c50d83673e1a4f212...|   1396750|[-25953.373, -310...|\n",
      "|d48b9e8d4cd618ece...|   1393571|[-24975.988, -375...|\n",
      "|0c46d3b537d4ca578...|   1393681|[-24756.7, -7474....|\n",
      "|9688e6409d379db64...|   1360715|[-20374.283, 1116...|\n",
      "|855d0affa7ac8f01a...|   1389268|[-20616.79, 6636....|\n",
      "|e26fa924ddcef1377...|   1389268|[-15911.65, -2736...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "subset_df count: 2435\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get a small subset of the dataset\n",
    "sub_species_df = (\n",
    "    dct_df.groupBy(\"species_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .where(F.col(\"n\") > 100)\n",
    "    .orderBy(F.rand(seed=42))\n",
    "    .limit(10)\n",
    ").cache()\n",
    "sub_species_df.show(truncate=80)\n",
    "\n",
    "# Collect the species_id into a list of values\n",
    "species_id_subset = [\n",
    "    row[\"species_id\"]\n",
    "    for row in sub_species_df.select(\"species_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "# Get subset of images to test pipeline\n",
    "subset_df = dct_df.where(F.col(\"species_id\").isin(species_id_subset)).cache()\n",
    "subset_df.show()\n",
    "\n",
    "# Count number of rows in subset_df\n",
    "print(f\"subset_df count: {subset_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 13:00:24 WARN CacheManager: Asked to cache already cached data.\n",
      "24/04/09 13:00:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF count: 1949\n",
      "Valid DF count: 486\n"
     ]
    }
   ],
   "source": [
    "# Perform a train-validation split\n",
    "train_df, valid_df = subset_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache the splits to improve performance\n",
    "train_df = train_df.cache()\n",
    "valid_df = valid_df.cache()\n",
    "\n",
    "print(f\"Train DF count: {train_df.count()}\")\n",
    "print(f\"Valid DF count: {valid_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | layer     | Linear              | 650   \n",
      "1 | accuracy  | MultilabelAccuracy  | 0     \n",
      "2 | f1_score  | MultilabelF1Score   | 0     \n",
      "3 | precision | MultilabelPrecision | 0     \n",
      "4 | recall    | MultilabelRecall    | 0     \n",
      "--------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgustine/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 61/61 [00:00<00:00, 106.46it/s, v_num=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 61/61 [00:00<00:00, 105.62it/s, v_num=12]\n"
     ]
    }
   ],
   "source": [
    "# Init params\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Prepare PyTorch datasets\n",
    "train_data = EmbeddingDataset(data=train_df, num_classes=num_classes)\n",
    "valid_data = EmbeddingDataset(data=valid_df, num_classes=num_classes)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model\n",
    "model = MultiLabelClassifier(\n",
    "    num_features=64, num_classes=num_classes\n",
    ")  # Only using 10 classses for testing\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=epochs)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
