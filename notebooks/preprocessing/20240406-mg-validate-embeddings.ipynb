{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stored filed in cloud bucket\n",
    "root = \"gs://dsgt-clef-plantclef-2024\"\n",
    "! date\n",
    "! gcloud storage ls {root}/data/process/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/parquet_files/\"\n",
    "train = \"PlantCLEF2024_training_cropped_resized_v2\"\n",
    "\n",
    "# Define the GCS path to the Train parquet file\n",
    "train_gcs_path = f\"{gcs_path}{train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "train_df = spark.read.parquet(train_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "train_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/process\"\n",
    "dino_emb_train = \"subset_training_cropped_resized_v2/dino/data\"\n",
    "dct_emb_train = \"subset_training_cropped_resized_v2/dino_dct/data\"\n",
    "\n",
    "# Define the GCS path to the embedding files\n",
    "dino_gcs_path = f\"{gcs_path}/{dino_emb_train}\"\n",
    "dct_gcs_path = f\"{gcs_path}/{dct_emb_train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "dino_df = spark.read.parquet(dino_gcs_path)\n",
    "dct_df = spark.read.parquet(dct_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "dino_df.show(n=5, truncate=50)\n",
    "dct_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with dino_df with train_df to get species names\n",
    "dino_joined_df = dino_df.join(train_df, \"image_name\", \"inner\").select(\n",
    "    [\n",
    "        dino_df[\"image_name\"],\n",
    "        train_df[\"species\"],\n",
    "        dino_df[\"species_id\"],\n",
    "        dino_df[\"dino_embedding\"],\n",
    "    ]\n",
    ")\n",
    "dino_joined_df.show(n=5)\n",
    "\n",
    "# Join dct_df with train_df to get species names\n",
    "dct_joined_df = dct_df.join(train_df, \"image_name\", \"inner\").select(\n",
    "    [\n",
    "        dct_df[\"image_name\"],\n",
    "        train_df[\"species\"],\n",
    "        dct_df[\"species_id\"],\n",
    "        dct_df[\"dct_embedding\"],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_images_from_embeddings\n",
    "\n",
    "# Plot DINO image embeddings\n",
    "plot_images_from_embeddings(\n",
    "    dino_joined_df, data_col=\"dino_embedding\", image_col=\"species\", grid_size=(3, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DCT image embeddings\n",
    "plot_images_from_embeddings(\n",
    "    dct_joined_df, data_col=\"dct_embedding\", image_col=\"species\", grid_size=(3, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full-size train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path and dataset names\n",
    "gcs_path = \"gs://dsgt-clef-plantclef-2024/data/process\"\n",
    "dino_emb_train = \"training_cropped_resized_v2/dino/data\"\n",
    "dct_emb_train = \"training_cropped_resized_v2/dino_dct/data\"\n",
    "\n",
    "# Define the GCS path to the embedding files\n",
    "dino_gcs_path = f\"{gcs_path}/{dino_emb_train}\"\n",
    "dct_gcs_path = f\"{gcs_path}/{dct_emb_train}\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "dino_df = spark.read.parquet(dino_gcs_path)\n",
    "dct_df = spark.read.parquet(dct_gcs_path)\n",
    "\n",
    "# Show the data\n",
    "dino_df.show(n=5, truncate=50)\n",
    "dct_df.show(n=5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
